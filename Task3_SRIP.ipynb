{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task3_SRIP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Task3"
      ],
      "metadata": {
        "id": "sEHJE6SJNYGC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J8D_quZNM8eR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset"
      ],
      "metadata": {
        "id": "TgYFtydSNJFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "X_data = np.concatenate((X_train, X_test), axis=0)\n",
        "y_data = np.concatenate((y_train, y_test), axis=0)\n",
        "print(X_data.shape,y_data.shape)\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
        "print(Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape)"
      ],
      "metadata": {
        "id": "LqdFTGakNKXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e117cf-a72a-443c-8126-bbd20a679966"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
            "(70000, 28, 28) (70000,)\n",
            "(56000, 28, 28) (56000,) (14000, 28, 28) (14000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Necessary Libraries"
      ],
      "metadata": {
        "id": "dNcvAnt8q1BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "from jax import grad, vmap, value_and_grad\n",
        "from sklearn.metrics import accuracy_score\n",
        "key = jax.random.PRNGKey(0)\n",
        "from tqdm import tqdm\n",
        "from jax.scipy.special import logsumexp\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nOMYc_dTqqdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp 1\n",
        "Tried one approach but probably used a wrong loss function so had poor prediction"
      ],
      "metadata": {
        "id": "MctggTiHi2f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class neural_network():\n",
        "  def __init__(self):\n",
        "    self.loss_ = []\n",
        "\n",
        "  def relu_activation(self,x): # relu activation\n",
        "    return jnp.maximum(0, x) \n",
        "\n",
        "  def forward(self,params,x): # computing forward propogation\n",
        "    hidden1, hidden2, last = params\n",
        "    x = self.relu_activation(jnp.dot(x,hidden1['weights'])+ hidden1['biases'])\n",
        "    x = self.relu_activation(jnp.dot(x,hidden2['weights'])+ hidden2['biases'])\n",
        "    x = jnp.dot(x,last['weights'])+ last['biases']\n",
        "    return x \n",
        "\n",
        "  def loss(self,params, x, y): # loss function\n",
        "    batch_forward = vmap(self.forward, in_axes=(None, 0), out_axes=0)\n",
        "    preds = batch_forward(params, x)\n",
        "    return -jnp.sum(preds*y)\n",
        "\n",
        "  def init_neuralnet_params(self,layer_dims): # initializing parameters\n",
        "    params = []\n",
        "    for i,(in_dim, out_dim) in enumerate(zip(layer_dims[:-1], layer_dims[1:])):\n",
        "      params.append(\n",
        "          dict(weights=jax.random.normal(jax.random.PRNGKey(i),(in_dim, out_dim)),\n",
        "              biases=jnp.ones(shape=(out_dim,))\n",
        "              )\n",
        "      )\n",
        "    return params\n",
        "\n",
        "\n",
        "  def update(self, params, x, y,lr): # updating the parameters\n",
        "    grads = jax.grad(self.loss)\n",
        "    grad_loss = grads(params, x, y)\n",
        "\n",
        "    # using jax.tree\n",
        "    return jax.tree_map(\n",
        "      lambda p, g: p - lr * g, params, grad_loss)\n",
        "    \n",
        "\n",
        "  def fit(self, X, y, batch_size=1, n_iter=150, lr=0.01, lr_type='constant'): # training the model\n",
        "\n",
        "    X = jnp.array(X)\n",
        "    X = jnp.reshape(X,(X.shape[0], 28*28))\n",
        "    y = jnp.array(y)\n",
        "\n",
        "    index = 0\n",
        "    params = self.init_neuralnet_params([784, 512, 512, 10])\n",
        "\n",
        "    for iter in tqdm(range(n_iter)):\n",
        "      if index >= X.shape[0]:\n",
        "        index = 0\n",
        "\n",
        "      if lr_type == 'inverse':\n",
        "                lr = lr / (iter+1)\n",
        "\n",
        "      sub_X = X[index:index+batch_size, :]\n",
        "      sub_y = y[index:index+batch_size]\n",
        "\n",
        "      params_updated =  self.update(params, sub_X, sub_y,lr)\n",
        "\n",
        "      self.loss_.append(self.loss(params, sub_X, sub_y))\n",
        "\n",
        "      index += batch_size\n",
        "\n",
        "    return params_updated\n",
        "\n",
        "  def predict_values(self, X): # prediting \n",
        "    X = jnp.array(X)\n",
        "    X = jnp.reshape(X,(X.shape[0], 28*28))\n",
        "\n",
        "    batch_forward = vmap(self.forward, in_axes=(None, 0), out_axes=0)\n",
        "    return batch_forward(params, X)\n",
        "\n",
        "  def predict(self,params,X):\n",
        "    preds = self.predict_values(X)\n",
        "    return np.argmax(preds, axis=1)\n",
        "\n",
        "  def plot_loss(self):\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    plt.plot(self.loss_)\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title('LossVsIterations')\n",
        "    plt.legend()\n",
        "    # plt.show()\n",
        "    plt.savefig('LossVsIterations.png')"
      ],
      "metadata": {
        "id": "Ugl7SYb1NOSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = neural_network()\n",
        "\n",
        "params = model.fit(Xtrain, ytrain, batch_size=10, n_iter=15000, lr=0.03, lr_type='constant')\n",
        "y_pred_test = model.predict(params, Xtest)\n",
        "y_pred_train = model.predict(params, Xtrain)\n",
        "acc_test = accuracy_score(ytest, y_pred_test)\n",
        "acc_train = accuracy_score(ytrain, y_pred_train)\n",
        "\n",
        "print(\"Test Accuracy for our model =\", acc_test)\n",
        "print(\"Train Accuracy for our model =\", acc_train)\n",
        "model.plot_loss()"
      ],
      "metadata": {
        "id": "Xk918C-zNPw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exp2 \n",
        "using cross entropy loss function and defining all required functions. Unable to predict due to some JAX issue which can be resolved. "
      ],
      "metadata": {
        "id": "SCoHO2Dgi4uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class neural_network1():\n",
        "  def __init__(self):\n",
        "    self.loss_ = []\n",
        "\n",
        "  def relu_activation(self, x):\n",
        "    return jnp.maximum(0, x)\n",
        "\n",
        "  def softmax(self, x):\n",
        "    return jnp.exp(x) / jnp.sum(jnp.exp(x), axis=0)\n",
        "\n",
        "  def forward(self,params,x): # forward pass across the network\n",
        "    for w, b in params[:-1]: # iterating over the first and inner layers\n",
        "      x = self.relu_activation(jnp.dot(x,w)+ b)\n",
        "    w, b = params[-1] # for the last layer\n",
        "    x = jnp.dot(x,w)+ b\n",
        "    return x \n",
        "\n",
        "  def batch_forward(self, params, x):\n",
        "    batch = vmap(self.forward, in_axes=(None, 0), out_axes=0)\n",
        "    return batch(params, x)\n",
        "\n",
        "  def loss(self, params, x, y): # computing the loss = -summation(y * log y-hat)\n",
        "    x_ = self.batch_forward(params, x)\n",
        "    y_pred = self.softmax(x_)\n",
        "    print(y_pred)\n",
        "    loss= -1* jnp.sum(jnp.dot(y,jnp.log(y_pred)))\n",
        "    return loss\n",
        "\n",
        "  def init_neuralnet_params(self,layer_dims): # randomly initializing the parameters for the network with the weights according to dimensions of layers\n",
        "    params = []\n",
        "    for i,(in_dim, out_dim) in enumerate(zip(layer_dims[:-1], layer_dims[1:])):\n",
        "      weights=jax.random.normal(jax.random.PRNGKey(i),(in_dim, out_dim))\n",
        "      biases=jnp.ones((out_dim,))\n",
        "      params.append((weights, biases))\n",
        "    return params\n",
        "  \n",
        "  def fit(self, X, y, batch_size=1, n_iter=150, lr=0.01, lr_type='constant'):\n",
        "\n",
        "    X = jnp.array(X)\n",
        "    X = jnp.reshape(X,(X.shape[0], 28*28))\n",
        "    y = jnp.array(y)\n",
        "\n",
        "    index = 0\n",
        "    params = self.init_neuralnet_params([784, 512, 512, 10])\n",
        "    grads = jax.grad(self.loss)\n",
        "\n",
        "    for iter in tqdm(range(n_iter)):\n",
        "      if index >= X.shape[0]:\n",
        "        index = 0\n",
        "\n",
        "      if lr_type == 'inverse':\n",
        "                lr = lr / (iter+1)\n",
        "\n",
        "      sub_X = X[index:index+batch_size, :]\n",
        "      sub_y = y[index:index+batch_size]\n",
        "\n",
        "      grad_loss = grads(params, sub_X, sub_y)\n",
        "      grad_loss_np = np.array(grad_loss)\n",
        "      params_np = np.array(params)\n",
        "      params_np = params_np - (lr * grad_loss_np)\n",
        "      self.loss_.append(self.loss(params, X, y))\n",
        "\n",
        "      index += batch_size\n",
        "\n",
        "    params_updated = params_np\n",
        "\n",
        "    return params_updated\n",
        "\n",
        "  def predict_values(self, X):\n",
        "    X = jnp.array(X)\n",
        "    X = jnp.reshape(X,(X.shape[0], 28*28))\n",
        "    return self.softmax(self.batch_forward(params, X))\n",
        "\n",
        "  def predict(self,params,X):\n",
        "    preds = self.predict_values(X)\n",
        "    # print(preds)\n",
        "    return np.argmax(preds, axis=1)\n",
        "\n",
        "  def plot_loss(self):\n",
        "    fig = plt.figure(figsize=(10,8))\n",
        "    plt.plot(self.loss_)\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title('LossVsIterations')\n",
        "    plt.legend()\n",
        "    # plt.show()\n",
        "    plt.savefig('LossVsIterations.png')"
      ],
      "metadata": {
        "id": "rq5wnfDDNSYd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = neural_network1()\n",
        "\n",
        "params = model.fit(Xtrain, ytrain, batch_size=256, n_iter=50, lr=0.03, lr_type='constant')\n",
        "y_pred_test = model.predict(params, Xtest)\n",
        "y_pred_train = model.predict(params, Xtrain)\n",
        "acc_test = accuracy_score(ytest, y_pred_test)\n",
        "acc_train = accuracy_score(ytrain, y_pred_train)\n",
        "\n",
        "print(\"Test Accuracy for our model =\", acc_test)\n",
        "print(\"Train Accuracy for our model =\", acc_train)\n",
        "model.plot_loss()"
      ],
      "metadata": {
        "id": "rd83h050NUaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References"
      ],
      "metadata": {
        "id": "3AKjVX07unNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n",
        "* https://jax.readthedocs.io/en/latest/notebooks/quickstart.html"
      ],
      "metadata": {
        "id": "dxvuTu7iu4pD"
      }
    }
  ]
}